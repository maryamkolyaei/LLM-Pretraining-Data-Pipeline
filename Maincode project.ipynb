{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b002e549-85b5-4178-8709-a9c7d8e5adf7",
   "metadata": {},
   "source": [
    "**Title:** Mainpipe Data Preparation Pipeline<br>\n",
    "**Author:** Mary Kolyaei<br>\n",
    "**Email:** maryamkolyaie@gmail.com<br>\n",
    "**Sources:** <br>\n",
    "AWS: https://aws.amazon.com/blogs/machine-learning/an-introduction-to-preparing-your-own-dataset-for-llm-training/<br>\n",
    "Hugging Face: https://huggingface.co/docs/datasets/en/quickstart<br>\n",
    "https://huggingface.co/docs/datasets/v1.4.0/loading_datasets.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af848829-39f3-484d-ae6a-e9e1aed13fa6",
   "metadata": {},
   "source": [
    "# Pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee80a344-c243-4488-a395-1a564b8740fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pandas...\n",
      "pandas installed successfully.\n",
      "\n",
      "Installing numpy...\n",
      "numpy installed successfully.\n",
      "\n",
      "Installing matplotlib...\n",
      "matplotlib installed successfully.\n",
      "\n",
      "Installing langdetect...\n",
      "langdetect installed successfully.\n",
      "\n",
      "Installing fastparquet...\n",
      "fastparquet installed successfully.\n",
      "\n",
      "Installing lxml...\n",
      "lxml installed successfully.\n",
      "\n",
      "Installing transformers...\n",
      "transformers installed successfully.\n",
      "\n",
      "Installing detoxify torch --quiet...\n",
      "Failed to install detoxify torch --quiet: Command '['pip', 'install', '-U', 'detoxify torch --quiet']' returned non-zero exit status 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pkg import install_packages\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb7c09-01ec-4a2d-a278-64dd12388751",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfa80dd-ab27-473c-8514-6c84637f90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging       # logging framework\n",
    "logger = logging.getLogger(\"stage8_sharding\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb5295-df5f-4edc-9a14-deba24b7466f",
   "metadata": {},
   "source": [
    "!python run_pipeline.py #executes every stage of the end-to-end data pipeline\n",
    "\n",
    "ðŸŒ± Stage 1 â€” Ingestion\n",
    "\n",
    "**Script:** `ingest.py`  \n",
    "**Input:** `mainpipe_data_v1.jsonl`  \n",
    "**Output:** `mainpipe_ingested_v1.parquet`\n",
    "\n",
    "### Steps\n",
    "- Loads raw `.jsonl` dataset  \n",
    "- Creates stable `doc_id` using SHA1 hash  \n",
    "- Adds ingestion timestamp (`ingest_ts`)  \n",
    "- Adds metadata fields (`source`)  \n",
    "- Saves an ingested, structured Parquet file\n",
    "\n",
    "\n",
    "ðŸ§¹ Stage 2 â€” Text Cleaning & Filtering\n",
    "\n",
    "**Script:** text_clean_and_filter.py\n",
    "**Input:** mainpipe_ingested_v1.parquet\n",
    "**Outputs:** mainpipe_cleaned_v2.parquet  &  mainpipe_cleaned_v2.jsonl  &  mainpipe_dropped_v2.parquet\n",
    "\n",
    "### Steps\n",
    "- Removes empty, whitespace & extremely short texts\n",
    "- Normalises whitespace & punctuation\n",
    "- Computes basic content stats\n",
    "- Basic filtering (empty, too short, etc.)\n",
    "\n",
    "\n",
    "ðŸ§½ Stage 3 â€” Deep Cleaning + PII Masking\n",
    "\n",
    "**Script:** deep_clean_and_pii.py\n",
    "**Input:** mainpipe_cleaned_v2.parquet\n",
    "**Outputs:** mainpipe_cleaned_v4.parquet  &  mainpipe_dropped_v4.parquet  &  mainpipe_cleaned_v4.jsonl\n",
    "\n",
    "### Steps\n",
    "- Removes HTML tags\n",
    "- Removes boilerplate (cookie banners, footers, disclaimers)\n",
    "- Normalises repeated characters\n",
    "- Detects and masks: <EMAIL>, <PHONE>, <CREDIT_CARD>, <IBAN>\n",
    "- Applies token-based heuristics:\n",
    "- stopword ratio\n",
    "- unique-token ratio\n",
    "- repetitive-token spam\n",
    "- Drops low-information documents\n",
    "\n",
    "\n",
    "ðŸ” Stage 4 â€” Deduplication (Exact + Near-Dup)\n",
    "\n",
    "**Script:** duplication.py\n",
    "**Input:** mainpipe_cleaned_v4.parquet\n",
    "**Outputs:** mainpipe_cleaned_v5.parquet  &  mainpipe_dropped_v5.parquet  &  mainpipe_cleaned_v5.jsonl\n",
    "\n",
    "### Steps\n",
    "- Builds canonical version of each text\n",
    "- Exact dedup: SHA256 hash\n",
    "- Near-dup: match first 500 chars of canonical text\n",
    "- Drops duplicate documents, drop_reason, etc\n",
    "\n",
    "\n",
    "â­ Stage 5 â€” Scoring & Mixture Assignment\n",
    "\n",
    "**Script:** scoring_and_mixture.py\n",
    "**Input:** mainpipe_cleaned_v5.parquet\n",
    "**Outputs:**  mainpipe_scored_v6.parquet  &  mainpipe_scored_v6.jsonl\n",
    "\n",
    "### Steps\n",
    "- Computes quality_score âˆˆ [0, 1] using:\n",
    "- language confidence\n",
    "- token count\n",
    "- unique-token ratio\n",
    "- PII penalty\n",
    "\n",
    "\n",
    "ðŸ§© Stage 6 â€” Tokenisation + Training JSONL Export\n",
    "\n",
    "**Script:** Tokenisation_JSONL_export.py\n",
    "**Input:** mainpipe_scored_v6.parquet\n",
    "**Outputs:** mainpipe_tokenised_v7.parquet  &  train_web_sample_tokenised.jsonl\n",
    "\n",
    "### Steps\n",
    "- Loads HuggingFace tokenizer (GPT-2)\n",
    "- input_ids\n",
    "- attention_mask\n",
    "- n_tokens\n",
    "- Drops extremely short or extremely long docs\n",
    "- Saves training-ready JSONL for model training\n",
    "\n",
    "\n",
    "ðŸ“¦ Stage 7 â€” Sharding\n",
    "\n",
    "**Script:** sharding.py\n",
    "**Input:** mainpipe_tokenised_v7.parquet\n",
    "**Outputs:** Shard files (e.g., train_shard_00001.jsonl, train_shard_00002.jsonl, â€¦)  &  manifest.json  &  tiny_train.jsonl\n",
    "\n",
    "### Steps\n",
    "- Splits tokenised dataset into smaller .jsonl chunks\n",
    "- Writes a manifest describing:\n",
    "- number of shards\n",
    "- number of docs\n",
    "- total tokens\n",
    "- tokenizer name\n",
    "- file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee00fec8-8915-492c-abc8-9a814a9feef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 18:21:01,060 - INFO - ==== Running full data pipeline ====\n",
      "2025-11-27 18:21:01,061 - INFO - Project root: C:\\Users\\mkolyaei\\Desktop\\Maincode\n",
      "2025-11-27 18:21:01,062 - INFO - ================================================================================\n",
      "2025-11-27 18:21:01,062 - INFO - Starting Stage 1: ingest:Raw JSONL â†’ Ingested Parquet pipeline\n",
      "2025-11-27 18:21:01,063 - INFO - Command: python ingest.py\n",
      "2025-11-27 18:21:15,265 - INFO - âœ… Stage 1: ingest:Raw JSONL â†’ Ingested Parquet pipeline completed successfully\n",
      "2025-11-27 18:21:15,265 - INFO - ================================================================================\n",
      "2025-11-27 18:21:15,265 - INFO - Starting Stage 2: text_clean_and_filter\n",
      "2025-11-27 18:21:15,270 - INFO - Command: python text_clean_and_filter.py\n",
      "2025-11-27 19:56:48,658 - INFO - âœ… Stage 2: text_clean_and_filter completed successfully\n",
      "2025-11-27 19:56:48,660 - INFO - ================================================================================\n",
      "2025-11-27 19:56:48,661 - INFO - Starting Stage 3: deep_clean_and_pii\n",
      "2025-11-27 19:56:48,661 - INFO - Command: python deep_clean_and_pii.py\n",
      "2025-11-27 20:00:54,771 - INFO - âœ… Stage 3: deep_clean_and_pii completed successfully\n",
      "2025-11-27 20:00:54,771 - INFO - ================================================================================\n",
      "2025-11-27 20:00:54,771 - INFO - Starting Stage 4: duplication\n",
      "2025-11-27 20:00:54,771 - INFO - Command: python duplication.py\n",
      "2025-11-27 20:02:03,099 - INFO - âœ… Stage 4: duplication completed successfully\n",
      "2025-11-27 20:02:03,102 - INFO - ================================================================================\n",
      "2025-11-27 20:02:03,103 - INFO - Starting Stage 5: scoring_and_mixture\n",
      "2025-11-27 20:02:03,105 - INFO - Command: python scoring_and_mixture.py\n",
      "2025-11-27 20:02:33,127 - INFO - âœ… Stage 5: scoring_and_mixture completed successfully\n",
      "2025-11-27 20:02:33,129 - INFO - ================================================================================\n",
      "2025-11-27 20:02:33,129 - INFO - Starting Stage 6: Tokenisation_JSONL_export\n",
      "2025-11-27 20:02:33,130 - INFO - Command: python Tokenisation_JSONL_export.py\n",
      "2025-11-27 20:06:05,692 - INFO - âœ… Stage 6: Tokenisation_JSONL_export completed successfully\n",
      "2025-11-27 20:06:05,694 - INFO - ================================================================================\n",
      "2025-11-27 20:06:05,695 - INFO - Starting Stage 7: sharding\n",
      "2025-11-27 20:06:05,695 - INFO - Command: python sharding.py\n",
      "2025-11-27 20:06:38,172 - INFO - âœ… Stage 7: sharding completed successfully\n",
      "2025-11-27 20:06:38,173 - INFO - ðŸŽ‰ Pipeline completed successfully. All stages finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Stage 6 scored parquet from: mainpipe_scored_v6.parquet\n",
      "Loading tokenizer: gpt2\n",
      "\n",
      "Token count stats (n_tokens):\n",
      "            n_tokens\n",
      "count  191507.000000\n",
      "mean      318.047909\n",
      "std       499.923922\n",
      "min         0.000000\n",
      "25%        85.000000\n",
      "50%       178.000000\n",
      "75%       358.000000\n",
      "max     16365.000000\n",
      "=== Stage 7: Token length filter summary ===\n",
      "Input rows           : 191507\n",
      "Kept rows            : 187855\n",
      "Dropped rows         : 3652\n",
      "\n",
      "Drop reasons (incl. previous stages if any):\n",
      "drop_reason\n",
      "too_many_tokens    2614\n",
      "too_few_tokens     1038\n",
      "Name: count, dtype: int64\n",
      "\n",
      ">>> Final training docs: 187855\n",
      ">>> Dropped (too short/long or earlier reasons): 3652\n",
      "\n",
      "Writing tokenised parquet to: mainpipe_tokenised_v7.parquet\n",
      "Writing JSONL to train_web_sample_tokenised.jsonl ...\n",
      "Done.\n",
      "\n",
      "Sample lines from tokenised JSONL:\n",
      "{\"input_ids\": [818, 262, 1239, 7464, 3344, 284, 5755, 12926, 286, 663, 36814, 7557, 11, 6283, 3996, 23299, 1666, 422, 262, 12926, 6355, 2055, 11, 257, 5863, 8489, 290, 2766, 422, 1111, 4671, 389, 18116, 284, 11508, 8850, 978, 2093, 504, 326, 24435, 262, 1814, 1276, 307, 262, 717, 2239, 13, 632, 815, 307, 262, 938, 290, 691, 706, 584, 4237, 286, 6426, 389, 3938, 18782, 13, 1081, 6542, 679, 320, 417, 7584, 366, 314, 4236, 326, 257, 9068, 4610, 815, 6211, 262, 36814, 7557, 12042, 11, 475, 938, 11, 407, 717, 13, 3274, 1100, 3137, 278, 262, 3056, 1687, 11, 584, 6426, 4237, 11, 26740, 15401, 12224, 11, 6630, 11, 290, 691, 706, 883, 815, 262, 7557, 338, 12042, 307, 3177, 13, 2312, 661, 389, 4684, 284, 1577, 1964, 3002, 284, 262, 4734, 11, 508, 423, 1464, 16563, 262, 36814, 7557, 6949, 13, 1148, 340, 1016, 284, 3520, 262, 2368, 6787, 286, 12926, 4819, 30, 18578, 1911], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"doc_id\": \"c2905a3d486e70377c494c55e709e60593d4dc6a\", \"url\": null, \"subset\": \"high_quality\", \"mixture_name\": \"web_sample\", \"quality_score\": 0.9450371734818839}\n",
      "{\"input_ids\": [1212, 1107, 373, 281, 10059, 9476, 13, 1649, 314, 2067, 3555, 11, 616, 5110, 4286, 286, 7595, 714, 407, 423, 587, 517, 1180, 621, 644, 314, 1043, 287, 428, 4998, 1621, 13, 2935, 24370, 286, 262, 7840, 25708, 11, 257, 3024, 11, 5894, 27268, 1956, 11, 6273, 33138, 351, 262, 37408, 27716, 286, 262, 7915, 3139, 286, 3311, 901, 290, 10730, 262, 44880, 286, 734, 15153, 11266, 416, 8468, 5917, 13, 5747, 4813, 4376, 290, 8776, 355, 15787, 301, 16746, 416, 511, 9214, 6972, 25949, 11, 389, 4191, 7428, 287, 3190, 1180, 11678, 13, 383, 7099, 6621, 6026, 89, 544, 447, 247, 82, 1621, 286, 3257, 3518, 30699, 290, 1842, 1626, 257, 4097, 286, 31665, 23716, 82, 1107, 7907, 616, 13843, 13, 314, 373, 30103, 3555, 546, 257, 1448, 286, 661, 2877, 24349, 11, 3190, 379, 262, 17703, 286, 262, 4847, 290, 262, 6590, 12766, 326, 13873, 326, 3572, 13, 2574, 23638, 447, 247, 82, 1204, 287, 262, 1748, 4329, 13354, 416, 4819, 2641, 290, 2354, 607, 1363, 13, 314, 1107, 8359, 428, 670, 286, 6754, 10165, 11, 4673, 517, 546, 7595, 1141, 262, 14062, 447, 247, 82, 290, 1542, 447, 247, 82, 13, 383, 3435, 547, 880, 7428, 290, 40945, 3690, 262, 1621, 389, 49610, 543, 2018, 38074, 41427, 544, 447, 247, 82, 35356, 11501, 351, 11658, 546, 1204, 290, 1692, 3450, 13, 1114, 502, 428, 1492, 7763, 262, 2818, 13516, 286, 32964, 11, 3968, 11, 6958, 11, 1641, 11, 4819, 290, 2106, 13], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"doc_id\": \"db55157cd8ee1de9d1c30f30ec4f700055cb17ab\", \"url\": \"http://leekat.booklikes.com/post/608842/an-unexpected-take-on-brazil\", \"subset\": \"high_quality\", \"mixture_name\": \"web_sample\", \"quality_score\": 0.942712410064228}\n",
      "{\"input_ids\": [4299, 3696, 7, 944, 2599, 37227, 25876, 287, 24458, 13, 7343, 286, 3706, 28047, 2374, 357, 7753, 6978, 11, 2546, 737, 1058, 81, 4906, 25, 1351, 58, 39286, 8979, 60, 37227, 3696, 796, 17635, 7508, 796, 2116, 13557, 7249, 13, 1136, 10786, 10951, 11537, 611, 407, 7508, 25, 1441, 3696, 611, 705, 16624, 6, 287, 7508, 25, 2779, 796, 7508, 17816, 3672, 20520, 329, 277, 287, 7508, 17816, 16624, 6, 5974, 3696, 13, 33295, 7, 39286, 8979, 7, 22179, 7, 8692, 11, 1635, 69, 17816, 6978, 20520, 828, 277, 17816, 13664, 20520, 4008, 2073, 25, 3696, 13, 33295, 7, 39286, 8979, 7, 10951, 17816, 3672, 6, 4357, 7508, 17816, 13664, 20520, 4008, 1441, 3696], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"doc_id\": \"563a1aa936b16400e89623cda9c7541d4b8061e3\", \"url\": \"https://github.com/idlesign/torrentool/blob/78c474c2ecddbad2e3287b390ac8a043957f3563/torrentool/torrent.py#L108-L130\", \"subset\": \"high_quality\", \"mixture_name\": \"web_sample\", \"quality_score\": 0.9581387225914828}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 20:06:38,314 - INFO - ==== Running full data pipeline ====\n",
      "2025-11-27 20:06:38,315 - INFO - Project root: C:\\Users\\mkolyaei\\Desktop\\Maincode\n",
      "2025-11-27 20:06:38,315 - INFO - ================================================================================\n",
      "2025-11-27 20:06:38,315 - INFO - Starting Stage 1: ingest:Raw JSONL \\u2192 Ingested Parquet pipeline\n",
      "2025-11-27 20:06:38,315 - INFO - Command: python ingest.py\n",
      "2025-11-27 20:06:53,201 - INFO - \\u2705 Stage 1: ingest:Raw JSONL \\u2192 Ingested Parquet pipeline completed successfully\n",
      "2025-11-27 20:06:53,201 - INFO - ================================================================================\n",
      "2025-11-27 20:06:53,201 - INFO - Starting Stage 2: text_clean_and_filter\n",
      "2025-11-27 20:06:53,201 - INFO - Command: python text_clean_and_filter.py\n",
      "2025-11-27 21:41:20,207 - INFO - \\u2705 Stage 2: text_clean_and_filter completed successfully\n",
      "2025-11-27 21:41:20,208 - INFO - ================================================================================\n",
      "2025-11-27 21:41:20,208 - INFO - Starting Stage 3: deep_clean_and_pii\n",
      "2025-11-27 21:41:20,208 - INFO - Command: python deep_clean_and_pii.py\n",
      "2025-11-27 21:41:21,400 - INFO - deep_clean_and_pii - Loading output from: mainpipe_cleaned_v2.parquet\n",
      "2025-11-27 21:41:21,400 - INFO - deep_clean_and_pii - Loading output from: mainpipe_cleaned_v2.parquet\n",
      "2025-11-27 21:41:23,145 - INFO - deep_clean_and_pii - \\u2714 Loaded Stage 2 cleaned parquet successfully\n",
      "2025-11-27 21:41:23,145 - INFO - deep_clean_and_pii - \\u2714 Loaded Stage 2 cleaned parquet successfully\n",
      "2025-11-27 21:45:07,500 - INFO - deep_clean_and_pii - === Stage 4: Deep cleaning & PII summary ===\n",
      "2025-11-27 21:45:07,500 - INFO - deep_clean_and_pii - === Stage 4: Deep cleaning & PII summary ===\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - Input rows           : 237677\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - Input rows           : 237677\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - Kept rows            : 237677\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - Kept rows            : 237677\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - Dropped rows         : 0\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - Dropped rows         : 0\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - Drop reasons (Stage 4 + previous): (no rows dropped)\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - Drop reasons (Stage 4 + previous): (no rows dropped)\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - PII stats (all rows):\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - PII stats (all rows):\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii -   Rows with has_pii = True: 9999\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii -   Rows with has_pii = True: 9999\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - === strip_html statistics ===\n",
      "2025-11-27 21:45:07,501 - INFO - deep_clean_and_pii - === strip_html statistics ===\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Total texts processed      : 237677\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Total texts processed      : 237677\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Texts that contained <...> : 66603\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Texts that contained <...> : 66603\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Texts actually changed     : 65587\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Texts actually changed     : 65587\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii - === Boilerplate removal statistics ===\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii - === Boilerplate removal statistics ===\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Documents processed        : 237677\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Documents processed        : 237677\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Documents with removal     : 2820\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Documents with removal     : 2820\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Total lines processed      : 237677\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Total lines processed      : 237677\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Total lines removed        : 2820\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Total lines removed        : 2820\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Percent of lines removed   : 1.19%\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii -   Percent of lines removed   : 1.19%\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii - === Token statistics summary ===\n",
      "2025-11-27 21:45:07,502 - INFO - deep_clean_and_pii - === Token statistics summary ===\n",
      "2025-11-27 21:45:07,507 - INFO - deep_clean_and_pii - token_count:     min=0  median=97  max=4997\n",
      "2025-11-27 21:45:07,507 - INFO - deep_clean_and_pii - token_count:     min=0  median=97  max=4997\n",
      "2025-11-27 21:45:07,512 - INFO - deep_clean_and_pii - unique_ratio:    min=0.000  median=0.728  max=1.000\n",
      "2025-11-27 21:45:07,512 - INFO - deep_clean_and_pii - unique_ratio:    min=0.000  median=0.728  max=1.000\n",
      "2025-11-27 21:45:07,516 - INFO - deep_clean_and_pii - stopword_ratio:  min=0.000  median=0.267  max=0.800\n",
      "2025-11-27 21:45:07,516 - INFO - deep_clean_and_pii - stopword_ratio:  min=0.000  median=0.267  max=0.800\n",
      "2025-11-27 21:45:07,518 - INFO - deep_clean_and_pii - Documents with low unique-token ratio (< 0.20): 0\n",
      "2025-11-27 21:45:07,518 - INFO - deep_clean_and_pii - Documents with low unique-token ratio (< 0.20): 0\n",
      "2025-11-27 21:45:07,518 - INFO - deep_clean_and_pii - Documents with high stopword ratio (> 0.95): 0\n",
      "2025-11-27 21:45:07,518 - INFO - deep_clean_and_pii - Documents with high stopword ratio (> 0.95): 0\n",
      "2025-11-27 21:45:07,518 - INFO - deep_clean_and_pii - Interpretation: token_count measures text length, unique_token_ratio measures vocabulary diversity, stopword_ratio measures proportion of 'function words'.\n",
      "2025-11-27 21:45:07,518 - INFO - deep_clean_and_pii - Interpretation: token_count measures text length, unique_token_ratio measures vocabulary diversity, stopword_ratio measures proportion of 'function words'.\n",
      "2025-11-27 21:45:07,518 - INFO - deep_clean_and_pii - Documents with low unique-token ratio (< 0.20): 0\n",
      "2025-11-27 21:45:07,518 - INFO - deep_clean_and_pii - Documents with low unique-token ratio (< 0.20): 0\n",
      "2025-11-27 21:45:07,519 - INFO - deep_clean_and_pii - Documents dropped for repetitive_token_spam: 0\n",
      "2025-11-27 21:45:07,519 - INFO - deep_clean_and_pii - Documents dropped for repetitive_token_spam: 0\n",
      "2025-11-27 21:45:07,573 - INFO - deep_clean_and_pii - \\u2714 Deep-clean + PII stage finished\n",
      "2025-11-27 21:45:07,573 - INFO - deep_clean_and_pii - \\u2714 Deep-clean + PII stage finished\n",
      "2025-11-27 21:45:07,574 - INFO - deep_clean_and_pii - Writing cleaned parquet to: mainpipe_cleaned_v4.parquet\n",
      "2025-11-27 21:45:07,574 - INFO - deep_clean_and_pii - Writing cleaned parquet to: mainpipe_cleaned_v4.parquet\n",
      "2025-11-27 21:45:14,666 - INFO - deep_clean_and_pii - \\u2714 Saved cleaned parquet (Stage 4 output)\n",
      "2025-11-27 21:45:14,666 - INFO - deep_clean_and_pii - \\u2714 Saved cleaned parquet (Stage 4 output)\n",
      "2025-11-27 21:45:14,666 - INFO - deep_clean_and_pii - Writing dropped parquet to: mainpipe_dropped_v4.parquet\n",
      "2025-11-27 21:45:14,666 - INFO - deep_clean_and_pii - Writing dropped parquet to: mainpipe_dropped_v4.parquet\n",
      "2025-11-27 21:45:14,675 - INFO - deep_clean_and_pii - \\u2714 Saved dropped-rows parquet for inspection\n",
      "2025-11-27 21:45:14,675 - INFO - deep_clean_and_pii - \\u2714 Saved dropped-rows parquet for inspection\n",
      "2025-11-27 21:45:14,675 - INFO - deep_clean_and_pii - Writing cleaned JSONL to: mainpipe_cleaned_v4.jsonl\n",
      "2025-11-27 21:45:14,675 - INFO - deep_clean_and_pii - Writing cleaned JSONL to: mainpipe_cleaned_v4.jsonl\n",
      "2025-11-27 21:45:28,726 - INFO - deep_clean_and_pii - All done.\n",
      "2025-11-27 21:45:28,726 - INFO - deep_clean_and_pii - All done.\n",
      "2025-11-27 21:45:29,763 - INFO - \\u2705 Stage 3: deep_clean_and_pii completed successfully\n",
      "2025-11-27 21:45:29,763 - INFO - ================================================================================\n",
      "2025-11-27 21:45:29,763 - INFO - Starting Stage 4: duplication\n",
      "2025-11-27 21:45:29,763 - INFO - Command: python duplication.py\n",
      "2025-11-27 21:45:30,522 - INFO - dedup_stage - Loading Stage 4 cleaned parquet from: mainpipe_cleaned_v4.parquet\n",
      "2025-11-27 21:45:30,522 - INFO - dedup_stage - Loading Stage 4 cleaned parquet from: mainpipe_cleaned_v4.parquet\n",
      "2025-11-27 21:45:33,337 - INFO - dedup_stage - \\u2714 Loaded Stage 4 cleaned parquet successfully\n",
      "2025-11-27 21:45:33,337 - INFO - dedup_stage - \\u2714 Loaded Stage 4 cleaned parquet successfully\n",
      "2025-11-27 21:46:15,325 - INFO - dedup_stage - === Stage 5: Deduplication summary ===\n",
      "2025-11-27 21:46:15,325 - INFO - dedup_stage - === Stage 5: Deduplication summary ===\n",
      "2025-11-27 21:46:15,326 - INFO - dedup_stage - Input rows           : 237677\n",
      "2025-11-27 21:46:15,326 - INFO - dedup_stage - Input rows           : 237677\n",
      "2025-11-27 21:46:15,326 - INFO - dedup_stage - Kept rows            : 191507\n",
      "2025-11-27 21:46:15,326 - INFO - dedup_stage - Kept rows            : 191507\n",
      "2025-11-27 21:46:15,326 - INFO - dedup_stage - Dropped rows         : 46170\n",
      "2025-11-27 21:46:15,326 - INFO - dedup_stage - Dropped rows         : 46170\n",
      "2025-11-27 21:46:15,326 - INFO - dedup_stage - Drop reasons (including dedup):\n",
      "2025-11-27 21:46:15,326 - INFO - dedup_stage - Drop reasons (including dedup):\n",
      "2025-11-27 21:46:15,328 - INFO - dedup_stage - \n",
      "drop_reason\n",
      "exact_duplicate    46053\n",
      "near_duplicate       117\n",
      "Name: count, dtype: int64\n",
      "2025-11-27 21:46:15,328 - INFO - dedup_stage - \n",
      "drop_reason\n",
      "exact_duplicate    46053\n",
      "near_duplicate       117\n",
      "Name: count, dtype: int64\n",
      "2025-11-27 21:46:15,329 - INFO - dedup_stage - Exact duplicate rows : 46053\n",
      "2025-11-27 21:46:15,329 - INFO - dedup_stage - Exact duplicate rows : 46053\n",
      "2025-11-27 21:46:15,330 - INFO - dedup_stage - Near-duplicate rows  : 117\n",
      "2025-11-27 21:46:15,330 - INFO - dedup_stage - Near-duplicate rows  : 117\n",
      "2025-11-27 21:46:15,497 - INFO - dedup_stage - \\u2714 Deduplication step finished\n",
      "2025-11-27 21:46:15,497 - INFO - dedup_stage - \\u2714 Deduplication step finished\n",
      "2025-11-27 21:46:15,498 - INFO - dedup_stage - Writing deduplicated cleaned parquet to: mainpipe_cleaned_v5.parquet\n",
      "2025-11-27 21:46:15,498 - INFO - dedup_stage - Writing deduplicated cleaned parquet to: mainpipe_cleaned_v5.parquet\n",
      "2025-11-27 21:46:24,177 - INFO - dedup_stage - \\u2714 Saved deduplicated cleaned parquet\n",
      "2025-11-27 21:46:24,177 - INFO - dedup_stage - \\u2714 Saved deduplicated cleaned parquet\n",
      "2025-11-27 21:46:24,177 - INFO - dedup_stage - Writing deduplicated dropped parquet to: mainpipe_dropped_v5.parquet\n",
      "2025-11-27 21:46:24,177 - INFO - dedup_stage - Writing deduplicated dropped parquet to: mainpipe_dropped_v5.parquet\n",
      "2025-11-27 21:46:25,166 - INFO - dedup_stage - Writing deduplicated cleaned JSONL to: mainpipe_cleaned_v5.jsonl\n",
      "2025-11-27 21:46:25,166 - INFO - dedup_stage - Writing deduplicated cleaned JSONL to: mainpipe_cleaned_v5.jsonl\n",
      "2025-11-27 21:46:36,741 - INFO - dedup_stage - Stage 5 complete.\n",
      "2025-11-27 21:46:36,741 - INFO - dedup_stage - Stage 5 complete.\n",
      "2025-11-27 21:46:36,741 - INFO - dedup_stage - Final docs after dedup: 191507\n",
      "2025-11-27 21:46:36,741 - INFO - dedup_stage - Final docs after dedup: 191507\n",
      "2025-11-27 21:46:36,741 - INFO - dedup_stage - Total dropped due to dedup (or earlier reasons in this stage): 46170\n",
      "2025-11-27 21:46:36,741 - INFO - dedup_stage - Total dropped due to dedup (or earlier reasons in this stage): 46170\n",
      "2025-11-27 21:46:37,801 - INFO - \\u2705 Stage 4: duplication completed successfully\n",
      "2025-11-27 21:46:37,801 - INFO - ================================================================================\n",
      "2025-11-27 21:46:37,802 - INFO - Starting Stage 5: scoring_and_mixture\n",
      "2025-11-27 21:46:37,802 - INFO - Command: python scoring_and_mixture.py\n",
      "2025-11-27 21:46:38,578 - INFO - stage6_scoring - Loading deduplicated parquet from: mainpipe_cleaned_v5.parquet\n",
      "2025-11-27 21:46:41,825 - INFO - stage6_scoring - \\u2714 Loaded Stage 5 parquet successfully\n",
      "2025-11-27 21:46:45,085 - INFO - stage6_scoring - === Stage 6: Scoring & mixture summary ===\n",
      "2025-11-27 21:46:45,085 - INFO - stage6_scoring - Rows scored: 191507\n",
      "2025-11-27 21:46:45,095 - INFO - stage6_scoring - Quality score stats:\n",
      "count    191507.000000\n",
      "mean          0.930002\n",
      "std           0.049164\n",
      "min           0.448081\n",
      "25%           0.917104\n",
      "50%           0.938595\n",
      "75%           0.958619\n",
      "max           1.000000\n",
      "Name: quality_score, dtype: float64\n",
      "2025-11-27 21:46:45,102 - INFO - stage6_scoring - Subset distribution:\n",
      "subset\n",
      "high_quality    187283\n",
      "rest              4224\n",
      "Name: count, dtype: int64\n",
      "2025-11-27 21:46:45,103 - INFO - stage6_scoring - \\u2714 Scoring & mixture assignment completed\n",
      "2025-11-27 21:46:45,103 - INFO - stage6_scoring - Writing scored parquet to: mainpipe_scored_v6.parquet\n",
      "2025-11-27 21:46:53,986 - INFO - stage6_scoring - Writing scored JSONL to: mainpipe_scored_v6.jsonl\n",
      "2025-11-27 21:47:07,505 - INFO - stage6_scoring - \\u2714 Saved scored parquet\n",
      "2025-11-27 21:47:07,505 - INFO - stage6_scoring - Stage Scoring & mixture complete.\n",
      "2025-11-27 21:47:08,404 - INFO - \\u2705 Stage 5: scoring_and_mixture completed successfully\n",
      "2025-11-27 21:47:08,404 - INFO - ================================================================================\n",
      "2025-11-27 21:47:08,404 - INFO - Starting Stage 6: Tokenisation_JSONL_export\n",
      "2025-11-27 21:47:08,404 - INFO - Command: python Tokenisation_JSONL_export.py\n",
      "2025-11-27 21:47:19,297 - INFO - stage6_scoring - \\u2714 Loaded Stage 6 parquet successfully\n",
      "2025-11-27 21:47:20,661 - INFO - stage6_scoring - \\u2714 Tokenizer loaded\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1317 > 1024). Running this sequence through the model will result in indexing errors\n",
      "2025-11-27 21:49:55,404 - INFO - stage6_scoring - \\u2714 Tokenisation completed\n",
      "2025-11-27 21:50:43,807 - INFO - \\u2705 Stage 6: Tokenisation_JSONL_export completed successfully\n",
      "2025-11-27 21:50:43,807 - INFO - ================================================================================\n",
      "2025-11-27 21:50:43,807 - INFO - Starting Stage 7: sharding\n",
      "2025-11-27 21:50:43,807 - INFO - Command: python sharding.py\n",
      "2025-11-27 21:50:44,613 - INFO - stage8_sharding - Loading tokenised parquet from: mainpipe_tokenised_v7.parquet\n",
      "2025-11-27 21:50:44,613 - INFO - stage8_sharding - Loading tokenised parquet from: mainpipe_tokenised_v7.parquet\n",
      "2025-11-27 21:50:48,272 - INFO - stage8_sharding - === Stage 8: Sharding & exports ===\n",
      "2025-11-27 21:50:48,272 - INFO - stage8_sharding - === Stage 8: Sharding & exports ===\n",
      "2025-11-27 21:50:48,272 - INFO - stage8_sharding - Total docs       : 187855\n",
      "2025-11-27 21:50:48,272 - INFO - stage8_sharding - Total docs       : 187855\n",
      "2025-11-27 21:50:48,273 - INFO - stage8_sharding - Docs per shard   : 50000\n",
      "2025-11-27 21:50:48,273 - INFO - stage8_sharding - Docs per shard   : 50000\n",
      "2025-11-27 21:50:48,273 - INFO - stage8_sharding - Number of shards : 4\n",
      "2025-11-27 21:50:48,273 - INFO - stage8_sharding - Number of shards : 4\n",
      "2025-11-27 21:50:48,273 - INFO - stage8_sharding -   Writing shard: shards\\train_shard_00001.jsonl\n",
      "2025-11-27 21:50:48,273 - INFO - stage8_sharding -   Writing shard: shards\\train_shard_00001.jsonl\n",
      "2025-11-27 21:50:55,263 - INFO - stage8_sharding -   Writing shard: shards\\train_shard_00002.jsonl\n",
      "2025-11-27 21:50:55,263 - INFO - stage8_sharding -   Writing shard: shards\\train_shard_00002.jsonl\n",
      "2025-11-27 21:51:02,371 - INFO - stage8_sharding -   Writing shard: shards\\train_shard_00003.jsonl\n",
      "2025-11-27 21:51:02,371 - INFO - stage8_sharding -   Writing shard: shards\\train_shard_00003.jsonl\n",
      "2025-11-27 21:51:09,459 - INFO - stage8_sharding -   Writing shard: shards\\train_shard_00004.jsonl\n",
      "2025-11-27 21:51:09,459 - INFO - stage8_sharding -   Writing shard: shards\\train_shard_00004.jsonl\n",
      "2025-11-27 21:51:14,960 - INFO - stage8_sharding - Writing manifest to: manifest.json\n",
      "2025-11-27 21:51:14,960 - INFO - stage8_sharding - Writing manifest to: manifest.json\n",
      "2025-11-27 21:51:14,961 - INFO - stage8_sharding - Manifest written. num_shards=4 total_docs=187855\n",
      "2025-11-27 21:51:14,961 - INFO - stage8_sharding - Manifest written. num_shards=4 total_docs=187855\n",
      "2025-11-27 21:51:14,961 - INFO - stage8_sharding - Manifest summary: num_shards=4 total_docs=187855 total_tokens=51857691\n",
      "2025-11-27 21:51:14,961 - INFO - stage8_sharding - Manifest summary: num_shards=4 total_docs=187855 total_tokens=51857691\n",
      "2025-11-27 21:51:14,972 - INFO - stage8_sharding - Creating toy shard with 1000 docs at: tiny_train.jsonl\n",
      "2025-11-27 21:51:14,972 - INFO - stage8_sharding - Creating toy shard with 1000 docs at: tiny_train.jsonl\n",
      "2025-11-27 21:51:14,972 - INFO - stage8_sharding -   Writing shard: tiny_train.jsonl\n",
      "2025-11-27 21:51:14,972 - INFO - stage8_sharding -   Writing shard: tiny_train.jsonl\n",
      "2025-11-27 21:51:15,114 - INFO - stage8_sharding - Sharding complete.\n",
      "2025-11-27 21:51:15,114 - INFO - stage8_sharding - Sharding complete.\n",
      "2025-11-27 21:51:15,792 - INFO - \\u2705 Stage 7: sharding completed successfully\n",
      "2025-11-27 21:51:15,793 - INFO - \\U0001f389 Pipeline completed successfully. All stages finished.\n"
     ]
    }
   ],
   "source": [
    "#executes every stage of the end-to-end data pipeline\n",
    "!python run_pipeline.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9495eb03-a4c6-4314-a29c-c7689989cf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 22:27:24,567 - INFO - Reading mainpipe_cleaned_v2.parquet\n",
      "2025-11-27 22:27:26,909 - INFO - Reading mainpipe_cleaned_v4.parquet\n",
      "2025-11-27 22:27:30,600 - INFO - Reading mainpipe_scored_v6.parquet\n",
      "2025-11-27 22:27:34,659 - INFO - Reading mainpipe_tokenised_v7.parquet\n",
      "2025-11-27 22:27:39,368 - INFO - Reading mainpipe_dropped_v2.parquet\n",
      "2025-11-27 22:27:39,630 - INFO - Reading mainpipe_dropped_v4.parquet\n",
      "2025-11-27 22:27:39,637 - INFO - Reading mainpipe_dropped_v5.parquet\n",
      "2025-11-27 22:27:40,318 - INFO - Saved plot: plots\\hist_n_tokens.png\n",
      "2025-11-27 22:27:40,511 - INFO - Saved plot: plots\\hist_char_len.png\n",
      "2025-11-27 22:27:40,691 - INFO - Saved plot: plots\\hist_word_count.png\n",
      "2025-11-27 22:27:40,845 - INFO - Saved plot: plots\\hist_lang_score.png\n",
      "2025-11-27 22:27:40,993 - INFO - Saved plot: plots\\hist_quality_score.png\n",
      "2025-11-27 22:27:41,149 - INFO - Saved plot: plots\\bar_drop_reasons.png\n",
      "2025-11-27 22:27:41,304 - INFO - Saved plot: plots\\hist_alpha_ratio.png\n",
      "2025-11-27 22:27:41,470 - INFO - Saved plot: plots\\hist_repetition_ratio.png\n",
      "2025-11-27 22:27:41,624 - INFO - Saved plot: plots\\hist_unique_token_ratio.png\n",
      "2025-11-27 22:27:41,845 - INFO - Saved plot: plots\\hist_stopword_ratio.png\n",
      "2025-11-27 22:27:41,937 - INFO - Saved plot: plots\\bar_exact_duplicates.png\n",
      "2025-11-27 22:27:42,031 - INFO - Saved plot: plots\\bar_near_duplicates.png\n",
      "2025-11-27 22:27:42,179 - INFO - Saved plot: plots\\bar_pii_email_hits.png\n",
      "2025-11-27 22:27:42,375 - INFO - Saved plot: plots\\bar_pii_phone_hits.png\n",
      "2025-11-27 22:27:42,520 - INFO - Saved plot: plots\\bar_pii_cc_hits.png\n",
      "2025-11-27 22:27:42,624 - INFO - Saved plot: plots\\bar_pii_iban_hits.png\n",
      "2025-11-27 22:27:42,726 - INFO - Saved PII summary plot\n",
      "2025-11-27 22:27:43,041 - INFO - Saved PII stats CSV: reports\\pii_stats.csv\n",
      "2025-11-27 22:27:43,046 - INFO - Saved drop_reason counts CSV: reports\\drop_reason_counts.csv\n",
      "2025-11-27 22:27:43,047 - INFO - Saved metrics JSON: reports\\metrics_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python plots_charts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347dbaa-7c26-45a3-9805-ea0e939a7e08",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
